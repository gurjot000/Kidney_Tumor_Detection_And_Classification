{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gurjot000/Kidney_Tumor_Detection_And_Classification/blob/main/Tumer_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pcylo2ntP66"
      },
      "source": [
        "## steps :\n",
        "\n",
        "1. read Data (excel file ) from google drive , images\n",
        "2. unzip images file\n",
        "3. read images\n",
        "4. preparation\n",
        "4. split data to  taring 80 % , testing 20%\n",
        "5. normalization images\n",
        "6. building model\n",
        "7. compile model\n",
        "8. fit model\n",
        "9. test model\n",
        "10. Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U5HDuCwTHbw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL8xnuhPS6u1"
      },
      "source": [
        "import seaborn as sns; sns.set(color_codes=True)  # visualization tool\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import statistics\n",
        "import collections\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from IPython.display import display\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import keras\n",
        "from keras.models import Sequential , Model\n",
        "from keras.layers import (\n",
        "                          Dense,\n",
        "                          Add,\n",
        "                          Conv2D,\n",
        "                          MaxPool2D,\n",
        "                          Flatten,\n",
        "                          Dropout,\n",
        "                          MaxPooling2D,\n",
        "                          Input,\n",
        "                          Conv2DTranspose,\n",
        "                          Concatenate,\n",
        "                          BatchNormalization,\n",
        "                          UpSampling2D,\n",
        "                          AveragePooling2D,\n",
        "                          GlobalAveragePooling2D,\n",
        "                          Activation,\n",
        "                          ZeroPadding2D\n",
        "                      )\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#from keras.optimizers import Adam , SGD\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import random\n",
        "import cv2\n",
        "from random import shuffle\n",
        "import itertools\n",
        "import shutil\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import imutils\n",
        "from tensorflow.keras import optimizers\n",
        "import cv2 as cv\n",
        "import seaborn as sns\n",
        "from random import choices\n",
        "from keras.applications.vgg16 import VGG16\n",
        "#from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet import ResNet50\n",
        "\n",
        "from keras.initializers import glorot_uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d1OqypKv1ZB"
      },
      "source": [
        "!unzip sample_data/images_data.zip -d sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E1nU36-gixc"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0fk-OdMUtHi"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAbNKAVzJ8TJ"
      },
      "source": [
        "patient_info = drive.CreateFile({'id':\"1cWmJm6-MPhhDMxhkBnQzMnANjVofmdG_\"})\n",
        "patient_info.GetContentFile(\"patient_info.csv\")\n",
        "patient_info = pd.read_csv(\"patient_info.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjMIdWRF8bpS"
      },
      "source": [
        "# Tumor Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faLq4wxlNPk2"
      },
      "source": [
        "patient_info[\"Tumor_label\"]= patient_info[\"Tumor_Type\"]\n",
        "cleanup_nums = {\n",
        "    \"Tumor_label\":{'Null':0  , \"Benign\":1 , 'Malignant':1 }\n",
        "}\n",
        "patient_info = patient_info.replace(cleanup_nums)\n",
        "del cleanup_nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0vz9SlpJ_iC"
      },
      "source": [
        "patient_info.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci64v9p0J_fP"
      },
      "source": [
        "patient_info.sample(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O3N1TmCJsrh"
      },
      "source": [
        "def crop_contour(image, plot=False):\n",
        "\n",
        "    # Convert the image to grayscale, and blur it slightly\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Threshold the image, then perform a series of erosions +\n",
        "    # dilations to remove any small regions of noise\n",
        "    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
        "    thresh = cv2.erode(thresh, None, iterations=2)\n",
        "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "\n",
        "    # Find contours in thresholded image, then grab the largest one\n",
        "    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cnts = imutils.grab_contours(cnts)\n",
        "    c = max(cnts, key=cv2.contourArea)\n",
        "\n",
        "    # Find the extreme points\n",
        "    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
        "    extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
        "    extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
        "    extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
        "\n",
        "    # crop new image out of the original image using the four extreme points (left, right, top, bottom)\n",
        "    new_image = image[extTop[1]:extBot[1], extLeft[0]:extRight[0]]\n",
        "\n",
        "    if plot:\n",
        "        plt.figure()\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(image)\n",
        "\n",
        "        plt.tick_params(axis='both', which='both',\n",
        "                        top=False, bottom=False, left=False, right=False,\n",
        "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
        "\n",
        "        plt.title('Original Image')\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(new_image)\n",
        "\n",
        "        plt.tick_params(axis='both', which='both',\n",
        "                        top=False, bottom=False, left=False, right=False,\n",
        "                        labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
        "        plt.title('Cropped Image')\n",
        "        plt.show()\n",
        "\n",
        "    return new_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqGSPOf_LKoV"
      },
      "source": [
        "labels = {0:\"Normal\" , 1:\"Tumor\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_10bAnQWJ_c2"
      },
      "source": [
        "def get_data (data_dir , target ):\n",
        "    X = list()\n",
        "    y=list()\n",
        "    img_size = 256\n",
        "    for index, row in patient_info.iterrows():\n",
        "        path = os.path.join(data_dir, str (row['Patient_Num']))\n",
        "        label = row[target]\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1]\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
        "\n",
        "                X.append(resized_arr)\n",
        "                y.append(label)\n",
        "            except Exception as e:\n",
        "                print(e , row['Patient_Num'] )\n",
        "    return X , y\n",
        "\n",
        "X , y  = get_data(\"sample_data/Dalia_Data/\", target = \"Tumor_label\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDWhnijVKUsI"
      },
      "source": [
        "dict(zip(list(y),[list(y).count(i) for i in list(y)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll8I0v29KUpG"
      },
      "source": [
        "plt.figure(figsize = (5,5))\n",
        "plt.imshow(X[20])\n",
        "plt.title(labels[y[20]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX8fLxe4KUm-"
      },
      "source": [
        "plt.figure(figsize = (5,5))\n",
        "plt.imshow(X[6000])\n",
        "plt.title(labels[y[6000]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz12NIkbKUkb"
      },
      "source": [
        "x_train, x_test, y_train , y_test = train_test_split(X, y, test_size = 0.20)\n",
        "x_train, x_val , y_train , y_val = train_test_split(x_train, y_train , test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMsrZNvULvGJ"
      },
      "source": [
        "print (\"Number images for training : {}\".format(len (x_train)) , dict(zip(list(y_train),[list(y_train).count(i) for i in list(y_train)])) )\n",
        "print (\"Number images for testing : {}\".format(len (x_test)), dict(zip(list(y_test),[list(y_test).count(i) for i in list(y_test)])) )\n",
        "print (\"Number images for Validation : {}\".format(len (x_val)) ,  dict(zip(list(y_val),[list(y_val).count(i) for i in list(y_val)])) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMzRO6a6LvDZ"
      },
      "source": [
        "def data_prepare (X , y , folder_name , labels ) :\n",
        "    path = \"sample_data/{}\".format(folder_name)\n",
        "    os.mkdir(path)\n",
        "    # create folder for labels\n",
        "    for key , value in labels.items()  :\n",
        "        path = \"sample_data/{}/{}\".format(folder_name,value)\n",
        "        os.mkdir(path)\n",
        "\n",
        "    if len (X) != len (y) :\n",
        "      print (\"error size data X and y is not equal\")\n",
        "      return\n",
        "\n",
        "    for index , value in enumerate(y) :\n",
        "      im = Image.fromarray(X[index])\n",
        "      path = \"sample_data/{}/{}/{}.jpeg\".format(folder_name,labels[value],str(index))\n",
        "      im.save(path)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Hvmgg0LvBI"
      },
      "source": [
        "data_prepare (X=x_train ,y=y_train ,folder_name=\"train\", labels=labels )\n",
        "data_prepare (X=x_test ,y=y_test ,folder_name=\"test\", labels=labels )\n",
        "data_prepare (X=x_val ,y=y_val ,folder_name=\"validation\", labels=labels )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8I0lAVuL8qL"
      },
      "source": [
        "## Genration Images\n",
        "\n",
        "train_datagen = ImageDataGenerator(shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "validation_datagen = ImageDataGenerator()\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('/content/sample_data/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'sparse')\n",
        "\n",
        "validation_set = validation_datagen.flow_from_directory('/content/sample_data/validation',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'sparse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgqGMdwVT8Sy"
      },
      "source": [
        "# my model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcRiLwjjaU7t"
      },
      "source": [
        "class AccuracyStopping(keras.callbacks.Callback):\n",
        "    def __init__(self, acc_threshold):\n",
        "        super(AccuracyStopping, self).__init__()\n",
        "        self._acc_threshold = acc_threshold\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        train_acc = logs.get('accuracy')\n",
        "        print(train_acc)\n",
        "        value=1-train_acc\n",
        "        print(value)\n",
        "        self.model.stop_training = value <= self._acc_threshold\n",
        "\n",
        "acc_callback = AccuracyStopping(0.02)\n",
        "\n",
        "def get_Model():\n",
        "    modelName= Sequential()\n",
        "    modelName.add(BatchNormalization(input_shape = (224,224,3)))\n",
        "    modelName.add(Conv2D(32, (3, 3), input_shape = (224, 224, 3), activation = 'relu'))\n",
        "    modelName.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    modelName.add(Dropout(0.25))\n",
        "    modelName.add(Flatten())\n",
        "    modelName.add(Dense(units = 128, activation = 'relu'))\n",
        "    modelName.add(Dense(units = 2, activation = 'softmax'))\n",
        "    return modelName\n",
        "\n",
        "x=get_Model()\n",
        "\n",
        "x.compile(\n",
        "    optimizer='adam' ,\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']    )\n",
        "\n",
        "history = x.fit(\n",
        "          training_set,\n",
        "          steps_per_epoch = (5376 /32),\n",
        "          epochs=50,\n",
        "          validation_data=validation_set,\n",
        "          validation_steps = (1344/32)\n",
        "                          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WkE4hguo-LN"
      },
      "source": [
        "x.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vedkMv9M8vj"
      },
      "source": [
        "print('Training Set Clases : ', training_set.class_indices )\n",
        "print(\"==\"*10)\n",
        "print('Validation Set Clases : ' , validation_set.class_indices )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz2H0BKHM8yS"
      },
      "source": [
        "loss,accuracy=x.evaluate(validation_set)\n",
        "print (f\"Test Loss     = {loss}\")\n",
        "print (f\"Test Accuracy = {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVsfBZpm9BGZ"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (15 ,15) , dpi=80,)\n",
        "ax.set_facecolor('#ffffff')\n",
        "ax.xaxis.label.set_color('#000000')\n",
        "ax.yaxis.label.set_color('#000000')\n",
        "ax.tick_params(axis='x', colors='#000000' )\n",
        "ax.tick_params(axis='y', colors='#000000')\n",
        "ax.spines['left'].set_color('#000000')\n",
        "ax.spines['bottom'].set_color('#000000')\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot( acc, label='Training Accuracy')\n",
        "plt.plot( val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot( loss, label='Training Loss')\n",
        "plt.plot( val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_184jXdX4FP"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "color=['#ff6600','#1976D2']\n",
        "\n",
        "path='/content/sample_data/test/Normal'\n",
        "l_Normal =[]\n",
        "\n",
        "filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]\n",
        "y_Normal =[0]*len(filelist)\n",
        "print (\"Number of images for Normal :\" , len (filelist))\n",
        "\n",
        "for img in filelist:\n",
        "  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  l_Normal.append(test_image)\n",
        "\n",
        "l_Normal_result=[]\n",
        "for i in range(len(l_Normal)):\n",
        "  xx = x.predict(l_Normal[i])\n",
        "  xx = np.round(xx).astype(int)\n",
        "  l_Normal_result.append(xx[0][1])\n",
        "\n",
        "l_Normal_draw=[]\n",
        "for i in range(len(l_Normal_result)):\n",
        "    if (l_Normal_result[i]== 0):\n",
        "        l_Normal_draw.append(\"Normal\")\n",
        "    else :\n",
        "        l_Normal_draw.append(\"Tumor\")\n",
        "\n",
        "display('==='*10)\n",
        "display(dict(zip(list(l_Normal_draw),[list(l_Normal_draw).count(i) for i in list(l_Normal_draw)])))\n",
        "display('==='*10)\n",
        "\n",
        "res = dict(zip(list(l_Normal_draw),[list(l_Normal_draw).count(i) for i in list(l_Normal_draw)]))\n",
        "labels = ['Normal','Tumor']\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (9 , 6) , dpi=80,)\n",
        "ax.set_facecolor('#ffffff')\n",
        "ax.xaxis.label.set_color('#000000')\n",
        "ax.yaxis.label.set_color('#000000')\n",
        "ax.tick_params(axis='x', colors='#000000' )\n",
        "ax.tick_params(axis='y', colors='#000000')\n",
        "ax.spines['left'].set_color('#000000')        # setting up Y-axis tick color to red\n",
        "ax.spines['bottom'].set_color('#000000')\n",
        "plt.bar( labels , res.values() ,width = 0.7,  color=[ '#ff6600', '#1976D2'] ,  align='center' , zorder=1)\n",
        "plt.xlabel('Label')\n",
        "plt.title('Tumer Detection Normal label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(l_Normal_draw)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwPj0YzdUJBk"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "path='/content/sample_data/test/Tumor'\n",
        "l_Tumor=[]\n",
        "\n",
        "filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]\n",
        "y_Tumor =[1]*len(filelist)\n",
        "print (\"Number of images for Tumor :\" , len (filelist))\n",
        "\n",
        "for img in filelist:\n",
        "  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  l_Tumor.append(test_image)\n",
        "\n",
        "l_Tumor_result=[]\n",
        "for i in range(len(l_Tumor)):\n",
        "  #xx = x.predict_classes(l_Tumor[i]\n",
        "  xx = x.predict(l_Tumor[i])\n",
        "  xx = np.round(xx).astype(int)\n",
        "  l_Tumor_result.append(xx[0][1])\n",
        "\n",
        "l_Tumor_draw=[]\n",
        "for i in range(len(l_Tumor_result)):\n",
        "    if (l_Tumor_result[i]== 0):\n",
        "        l_Tumor_draw.append(\"Normal\")\n",
        "    else :\n",
        "        l_Tumor_draw.append(\"Tumor\")\n",
        "\n",
        "display('==='*10)\n",
        "display(dict(zip(list(l_Tumor_draw),[list(l_Tumor_draw).count(i) for i in list(l_Tumor_draw)])))\n",
        "display('==='*10)\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(l_Tumor_draw)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cosgVMfgX4FQ"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "print('Training Set Clases')\n",
        "print(training_set.class_indices)\n",
        "print('Testing Set Clases')\n",
        "print(validation_set.class_indices)\n",
        "print(\"======\"*10)\n",
        "print('\\nConfusion Matrix')\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Tumor']\n",
        "\n",
        "y_labels  = y_Normal +y_Tumor\n",
        "x_results = l_Normal_result + l_Tumor_result\n",
        "print(classification_report( y_labels , x_results , target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21L0ldb3aU9W"
      },
      "source": [
        "x.save('/content/drive/MyDrive/Tumer_classification/best_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHDMMo0BTELz"
      },
      "source": [
        "# ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m7k71_VY9n6"
      },
      "source": [
        "#add RESNet Model as a layer in ou model as we use in the structure above\n",
        "\n",
        "stop = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=6\n",
        ")\n",
        "\n",
        "base_model_2 = Sequential()\n",
        "base_model_2.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
        "base_model_2.add(Dropout(0.25))\n",
        "base_model_2.add(Flatten())\n",
        "base_model_2.add(Dense(units = 128, activation = 'relu'))\n",
        "base_model_2.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "base_model_2.compile(\n",
        "    optimizer='adam' ,\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC6d7sTxsfaK"
      },
      "source": [
        "base_model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19mxp1RlY_VP"
      },
      "source": [
        "history=base_model_2.fit(training_set,validation_data=validation_set,epochs=50,callbacks=[stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxkUMFsfsvBA"
      },
      "source": [
        "print('Training Set Clases : ', training_set.class_indices )\n",
        "print(\"==\"*10)\n",
        "print('Validation Set Clases : ' , validation_set.class_indices )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YuyNl9nsvBg"
      },
      "source": [
        "loss,accuracy=base_model_2.evaluate(validation_set)\n",
        "print (f\"Test Loss     = {loss}\")\n",
        "print (f\"Test Accuracy = {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcQQWed0svBh"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot( acc, label='Training Accuracy')\n",
        "plt.plot( val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot( loss, label='Training Loss')\n",
        "plt.plot( val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OeFnNF8svBh"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "path='/content/sample_data/test/Normal'\n",
        "l_Normal =[]\n",
        "\n",
        "filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]\n",
        "y_Normal =[0]*len(filelist)\n",
        "print (\"Number of images for Normal :\" , len (filelist))\n",
        "\n",
        "for img in filelist:\n",
        "  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  l_Normal.append(test_image)\n",
        "\n",
        "l_Normal_result=[]\n",
        "for i in range(len(l_Normal)):\n",
        "  xx= base_model_2.predict(l_Normal[i])\n",
        "  xx = np.round(xx).astype(int)\n",
        "  l_Normal_result.append(xx[0][1])\n",
        "\n",
        "\n",
        "l_Normal_draw=[]\n",
        "for i in range(len(l_Normal_result)):\n",
        "    if (l_Normal_result[i] == 0):\n",
        "        l_Normal_draw.append(\"Normal\")\n",
        "    else :\n",
        "        l_Normal_draw.append(\"Tumor\")\n",
        "\n",
        "display('==='*10)\n",
        "display(dict(zip(list(l_Normal_draw),[list(l_Normal_draw).count(i) for i in list(l_Normal_draw)])))\n",
        "display('==='*10)\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(l_Normal_draw)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk2c55x2eTsn"
      },
      "source": [
        "xx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKffdTlysvBi"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "path='/content/sample_data/test/Tumor'\n",
        "l_Tumor=[]\n",
        "\n",
        "filelist= [file for file in os.listdir(path) if file.endswith('.jpeg')]\n",
        "y_Tumor =[1]*len(filelist)\n",
        "print (\"Number of images for Tumor :\" , len (filelist))\n",
        "\n",
        "for img in filelist:\n",
        "  test_image = image.load_img(os.path.join(path, img),target_size = (224, 224))\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  l_Tumor.append(test_image)\n",
        "\n",
        "l_Tumor_result=[]\n",
        "for i in range(len(l_Tumor)):\n",
        "  xx= base_model_2.predict(l_Tumor[i])\n",
        "  xx = np.round(xx).astype(int)\n",
        "  l_Tumor_result.append(xx[0][1])\n",
        "\n",
        "l_Tumor_draw=[]\n",
        "for i in range(len(l_Tumor_result)):\n",
        "    if (l_Tumor_result[i]== 0):\n",
        "        l_Tumor_draw.append(\"Normal\")\n",
        "    else :\n",
        "        l_Tumor_draw.append(\"Tumor\")\n",
        "\n",
        "display('==='*10)\n",
        "display(dict(zip(list(l_Tumor_draw),[list(l_Tumor_draw).count(i) for i in list(l_Tumor_draw)])))\n",
        "display('==='*10)\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(l_Tumor_draw)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtpEvKnEsvBi"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "print('Training Set Clases')\n",
        "print(training_set.class_indices)\n",
        "print('Testing Set Clases')\n",
        "print(validation_set.class_indices)\n",
        "print(\"======\"*10)\n",
        "print('\\nConfusion Matrix')\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Tumor']\n",
        "\n",
        "y_labels  = y_Normal +y_Tumor\n",
        "x_results = l_Normal_result + l_Tumor_result\n",
        "print(classification_report( y_labels , x_results , target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}